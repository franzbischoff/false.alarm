---
editor_options:
  markdown:
    canonical: true
    mode: markdown
    wrap: 120
---

<!--
This is for including Chapter 1. Notice that it's also good practice to name your chunk. This will
help you debug potential issues as you knit. The chunk above is called intro and the one below is
called chapter1. Feel free to change the name of the Rmd file as you wish, but don't forget to
change it here from chap1.Rmd.
-->

# Scientific contributions

## Matrix Profile

Since the first paper presenting this new concept [@Yeh2017a], lots of investigations were made to
speed up its computation. It is notable how all computations are not dependent on the _rolling
window size_ as previous works not using Matrix Profile. Aside from this, we can see that the first
STAMP [@Yeh2017a] algorithm has the time complexity of $O(n^2log{n})$ while STOMP [@zhu2016]
$O(n^2)$ (a significant improvement), but STOMP lacks the "any-time" property. Later SCRIMP
[@zhu2018] solves this problem keeping the same time complexity of $O(n^2)$. Here we are in the
"exact" algorithms domain and we will not extend the scope for conciseness.

The main issue with the algorithms above is the dependency on a fast Fourier transform (FFT)
library. FFT has been extensively optimized and architecture/CPU bounded to exploit the most of
speed. Also, padding data to some power of 2 happens to increase the efficiency of the
algorithm. We can argue that time complexity doesn't mean "faster" when we can exploit low-level
instructions. In our case, using FFT in a low-power device is overkilling. For example, a quick
search over the internet gives us a hint that computing FFT on a 4096 data in an ESP32 takes
about 21ms (~47 computations in 1 second). This means ~79 seconds for computing all FFT's
(~3797) required for STAMP using a window of 300. Currently, we can compute a full matrix of 5k
data in about 9 seconds in an ESP32 MCU (Fig. \@ref(fig:esp32)), and keep updating it as fast as
1 min of data (at 250hz) in just 6 seconds.

Recent works using _exact_ algorithms are using an unpublished algorithm called **MPX**, which
computes the Matrix Profile using cross-correlation methods ending up faster and is easily
portable.

**On computing the Matrix Profile:** the contribution of this work on this area is adding the
*Online* capability to MPX, which means we can update the Matrix Profile as new data comes in.

**On extending the Matrix Profile:** the contribution of this work on this area is the use of an
unexplored constraint that we could apply on building the Matrix Profile we are calling _Similarity
Threshold_ (ST). The original work outputs the similarity values in Euclidean Distance (ED) values,
while MPX naturally outputs the values in Pearson's correlation coefficients (CC). Both ED and CC
are interchangeable using the equation \@ref(eq:edcc). However, we may argue that it is easier to
compare values that do not depend on the window size during an exploratory phase. MPX happens to
naturally return values in CC, saving a few more computation time. The ST is an interesting factor
that we can use, especially when detecting pattern changes during time. The FLOSS algorithm relies
on counting references between indexes in the time series. ST can help remove "noise" from these
references since only similar patterns above a certain threshold are referenced, and changes have
more impact on these counts. The best ST threshold is still to be determined.

\
\begin{equation}
CC = 1 - \frac{ED}{(2 \times WindowSize)} (\#eq:edcc)
\end{equation}
\

## Regime change detection

In the original paper, in chapter 3.5, the authors of FLOSS wisely introduce the **temporal
constraint**, which improves the sensitivity of regime change detection on situations where
a regime may alternate in short periods of time.

Nevertheless, the authors declare the correction curve typically used on the algorithm as "simply a
uniform distribution", but this is not an accurate statement. the _Arc Counts_ of newly incoming
data are truncated by the same amount of temporal constraint. This prevents completely the detection
of a regime change in the last 10 seconds as this thesis requires.

The main contribution of this work on this area is overcoming this issue by computing the
theoretical distribution using the temporal constraint parameters beforehand. as shown in Fig.
\@ref(fig:distributions). That gives us enough data to evaluate a regime change accurately utilizing
a minimum of $2 \times WindowSize$ datapoints.

```{r dist-data, message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}
source(here("scripts", "common", "compute_floss.R"))

get_dist <- function(mp_const = 1250, floss_const = 0) {
  set.seed(2021)
  iac <- list()
  pro_size <- 5000
  mp_time_constraint <- mp_const
  floss_time_constraint <- floss_const
  for (i in 1:100) {
    iac[[i]] <- get_asym(pro_size, mp_time_constraint, floss_time_constraint)
  }

  aic_avg <- rowMeans(as.data.frame(iac))

  data.frame(index = 1:5000, counts = aic_avg)
}

data_5000 <- get_dist(5000)
data_4250 <- get_dist(4250)
data_2500 <- get_dist(2500)
data_1250 <- get_dist(1250)

floss_data_5000 <- get_dist(0, 5000)
floss_data_4250 <- get_dist(0, 4250)
floss_data_2500 <- get_dist(0, 2500)
floss_data_1250 <- get_dist(0, 1250)
```

```{r distributions, echo=FALSE, fig.cap="1D-IAC distributions for earlier temporal constraint (on Matrix Profile)", message=FALSE, warning=FALSE}

floss_dist <- ggplot(data_5000, aes(index, counts)) +
  geom_line(size = 0.1) +
  ggtitle("a) No constraint") +
  theme_grey(base_size = 7)

floss_4250 <- ggplot(data_4250, aes(index, counts)) +
  geom_line(size = 0.1) +
  ggtitle("b) Constraint of 4250") +
  theme_grey(base_size = 7)

floss_2500 <- ggplot(data_2500, aes(index, counts)) +
  geom_line(size = 0.1) +
  annotate("segment", y = 0, yend = max(data_2500$counts), x = 2500, xend = 2500, linetype = 2, size = 0.1) +
  annotate("text", x = 2500 - 80, y = 40, label = "start", color = "black", size = 2, angle = 90, hjust = 0) +
  annotate("segment", y = 0, yend = max(data_2500$counts), x = 5000 - 2500 * 0.9, xend = 5000 - 2500 * 0.9, linetype = 2, size = 0.1) +
  annotate("text", x = 5000 - 2500 * 0.9 - 80, y = 40, label = "end", color = "black", size = 2, angle = 90, hjust = 0) +
  ggtitle("c) Constraint of 2500") +
  theme_grey(base_size = 7)

floss_1250 <- ggplot(data_1250, aes(index, counts)) +
  geom_line(size = 0.1) +
  annotate("segment", y = 0, yend = max(data_1250$counts), x = 1250, xend = 1250, linetype = 2, size = 0.1) +
  annotate("text", x = 1250 - 80, y = 40, label = "start", color = "black", size = 2, angle = 90, hjust = 0) +
  annotate("segment", y = 0, yend = max(data_1250$counts), x = 5000 - 1250 * 0.9, xend = 5000 - 1250 * 0.9, linetype = 2, size = 0.1) +
  annotate("text", x = 5000 - 1250 * 0.9 - 80, y = 40, label = "end", color = "black", size = 2, angle = 90, hjust = 0) +
  ggtitle("d) Constraint of 1250") +
  theme_grey(base_size = 7)

gg <- gridExtra::arrangeGrob(floss_dist, floss_4250, floss_2500, floss_1250,
  nrow = 2,
  bottom = grid::textGrob(paste("The plot a) shows the distribution used for the arc count correction when there is no time constraint.", "\n", "b) Shows a constraint of 3/4 of the total. c) 1/2 of the total. d) 1/4 of the total; here we see clearly the flat line.", "\n", "The dashed line marks the start and the end of the uniform zone."), just = "center", gp = grid::gpar(fontsize = 7))
)

grid::grid.draw(gg)
```

