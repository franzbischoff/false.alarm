---
title: "Real-time Detection of ECG Signal Change for Low-Resource Environments: Utilizing the FLOSS Algorithm of the Matrix Profile"
author: ["Francisco Bischoff", "Pedro Rodrigues", "Eamonn Keogh"]
link-citations: true
bibliography: [../references.bib]
csl: ../../thesis/csl/ama.csl
linestretch: 1.5
indent: true
reference-location: document
citation-location: document
execute:
  cache: true
format:
  html:
    theme:
      dark: [darkly, dark.scss]
      light: [flatly, light.scss]
    highlight-style: oblivion
    date: '2023-11-22'
    fontsize: "1.0em"
    code-fold: true
    html-math-method: katex
    toc: true
    toc-title: Contents
    toc-expand: 2
    toc-depth: 3
    # toc-location: right
    # css: style.css
    citeproc: true
    standalone: true
    embed-resources: true
    number-sections: true
    number-depth: 3
    # keep-tex: true
    link-external-icon: true
    link-external-newwindow: true
    anchor-sections: true
    smooth-scroll: true
  pdf:
    theme:
      dark: [darkly, dark.scss]
      light: [flatly, light.scss]
    date: '2023-11-22'
    toc: true
    toc-title: Contents
    toc-depth: 3
    fig-pos: "H"
    citeproc: true
    citecolor: black
    geometry:
      - top=1in
      - left=1in
      - right=1in
      - bottom=1in
    documentclass: scrartcl
    number-sections: true
    crossref:
      fig-prefix: Fig.
    number-depth: 3
    colorlinks: true
    highlight-style: tango

editor:
  render-on-save: true
  markdown:
    wrap: none
    canonical: true
---

```{r setup, include=FALSE}

source(here::here("scripts", "common", "read_ecg.R"))
source(here::here("scripts", "common", "score_floss.R"))
source(here::here("scripts", "helpers", "plot_fluss.R"))
```

# Focus {.unnumbered}

Article 1 focuses on the detection part of the algorithmic sequence, while Article 2 explores the classification aspect. The methods section in Article 2 should refer back to Article 1 to explain how the entire algorithmic sequence is constructed. Both articles can share similar application scenarios, building a coherent narrative that connects them.

By structuring your research into these two distinct articles, you can comprehensively present your innovative work while clearly defining the respective contributions of detection and classification within your overall algorithmic pipeline.

Focus on (choices):

-   **development and implementation** of the algorithmic pipeline for detecting life-threatening heart rates using the FLOSS algorithm of the Matrix Profile. You can **discuss the challenges and considerations in designing the pipeline to be suitable for low computational power equipment** and low memory devices such as wearable devices. Additionally, you can present the **results of the detection performance** and **discuss the potential applications and implications** of this pipeline in real-time ECG monitoring.

-   the detection of ECG signal change using FLOSS. The article can discuss the challenges in detecting life-threatening heart rate patterns in real-time ECG monitoring and how the Matrix Profile algorithm, specifically FLOSS, can effectively address these challenges. Additionally, the article can highlight the advantages of using low computational power equipment and low memory, such as wearable devices, for point-of-care ECG monitoring.

-   presents a novel algorithmic pipeline for the real-time detection of life-threatening heart rates in Point-of-Care (POC) ECG using the FLOSS algorithm. The pipeline is designed to be used in low computational power equipment and low memory, such as wearable devices. The article discusses the implementation details and performance evaluation of the pipeline, demonstrating its potential for accurate and timely detection of life-threatening cardiac rhythms in POC ECG monitoring.

This article can focus on the foundational aspect of your research, outlining the mechanisms for detecting changes in ECG signals, especially in low computational power environments such as wearables.

This article could discuss the need for arrhythmia detection in wearable devices, as well as the challenges of developing an algorithm that is both accurate and efficient enough to run on a wearable device. The article could then describe your algorithmic pipeline in detail, including the two main steps of regime change detection and classification. The article would conclude by discussing the results of your evaluation, as well as the potential impact of your work on the development of wearable arrhythmia detection devices.

[@kowsar2022] points to the next DOI and says that the MP is too dependent on the window size "In a closely related approach to motif discovery methods, Gharghabi *et al*. [@gharghabi2018] designed FLOSS to identify boundaries of repetitive regions in a time series. FLOSS is based on the matrix profile (MP), an algorithm that can detect motifs from time series in an unsupervised manner using a sliding window of size M. Mirmomeni *et al*. [@mirmomeni2018] showed that both MP and FLOSS are highly sensitive to their key input parameter M for finding consecutive repeating regions in a time series. They demonstrated that for each different IoR, we must use a different M , which prevents FLOSS from being a one-pass and real-time algorithm. Mirmomeni *et al*. [@mirmomeni2018] designed a method for finding the best M to identify a repeating region from a time series. However, we still need to apply their method using multiple passes in order to find all IoR in a time series. Therefore, their method is not suited to real-time online applications where the computation must occur on a resource-constrained device, such as a wearable. See Section IX-A for a detailed explanation of this method."

[@mirmomeni2018] Says that MP is too dependent on the window size \[but, window_size too high, despite. And they don't use a buffer size, they use the whole data!\]

[@lin2016] Proposes evaluation technique F1

# Abstract {.unnumbered}

asdasd

This article will focus on the use of the Matrix Profile framework, particularly the FLOSS algorithm, in detecting changes in real-time one-lead ECG signals. This serves as the foundation for the subsequent classification of the signal changes in terms of severity. f \# Introduction

# Introduction

> Background of electrocardiogram analysis.

Currently, Point-of-Care (POC) ECG monitoring works either as plot devices or alarms for abnormal cardiac rhythms using predefined normal trigger ranges. Modern devices also incorporate algorithms to analyze arrhythmias, improving their specificity. On the other hand, full 12-derivation ECG machines are complex, are not suited to use as simple monitors, and are used with strict techniques for formal diagnostics of hearth electric conduction pathologies. The automatic diagnostics are derived from a complete analysis of the 12-dimension data after it is fully and well collected. Both systems do not handle disconnected leads and patient's motions, being strictly necessary to have a good and stable signal to allow proper diagnosis. These interferences with the data collection frequently originate false alarms, increasing both patient and staff's stress; depending on how it is measured, the rate of false alarms (overall) in ICU is estimated at 65 to 95% [@donchin2002].

In February of 2015, the CinC/Physionet Challenge 2015 was about "Reducing False Arrhythmia Alarms in the ICU" [@Clifford2015]. The introduction article stated that it had been reported that up to 86% of the alarms are false, and this can lead to decreased staff attention and an increase in patients' delirium [@Lawless1994; @Chambrin2001; @Parthasarathy2004]. This subject draws attention to the importance of correctly identifying abnormal heart electric patterns in order to avoid the overload of clinical staff. Meanwhile, this opens the opportunity to think outside the ICU setting, where we still monitor patients (and ourselves) using devices with low processing power, for example ward monitors, home devices and wearable devices.

> Importance of real-time detection for severe heart rates.

Real-time monitoring of life-threatening ECG patterns in high-risk patients (for example, those with cardiac implantable electronic devices (CIEDs), such as pacemakers, implantable cardioverter defibrillators (ICDs), biventricular pacemakers, and cardiac loop recorders), is a crucial aspect of remote patient monitoring. These devices provide invaluable data regarding a patient's heart activity but often rely on the accuracy of retrospective evaluation of recorded data for any abnormalities. Early detection enables timely intervention, potentially preventing serious complications and reducing morbidity and mortality rates. Furthermore, real-time data transmission from wearable devices to healthcare providers can facilitate rapid decision-making and treatment initiation, particularly when considering the rapid progression and potentially life-threatening nature of certain cardiac events. Furthermore, remote patient monitoring systems also provide a valuable source of longitudinal data that can be used for predictive modeling and risk stratification, helping healthcare providers tailor personalized treatment plans for individual patients. Real-time monitoring may not only enhance patient safety but also improve the efficiency and effectiveness of care delivery, making it an essential component of modern cardiovascular management strategies.

> Existing challenges in using low computational power equipment.

The challenge starts with developing an algorithm that not only has acceptable accuracy but is also efficient to run in the constricted environment of a wearable device. Some effort has been taken to analyze wearable sensor data. Kowsar, *et al.*[@kowsar2022] made use of accelerometers to detect and track repeating patterns in the context of exercising. Bhanushali, *et al.*[@bhanushali2020] uses a custom circuit to detect stress/no stress detection using a chest ECG device, Grammatikakis, *et al.*[@grammatikakis2021] uses a Body Gateway sensor to transmit the ECG data for remote processing and analysis and Cheng, *et al.* [@cheng2020] which proposes a multi-label classification of arrhythmias from pre-compressed ECG signal using Neural Networks.

> Significance of real-time detection in wearable devices Brief introduction to the Matrix Profile framework and the importance of the FLOSS algorithm

Here, we propose a procedure to monitor a patient's ECG in real-time, online (not in batches), using a constricted environment (low CPU and low memory). This first work will focus on detecting *semantic* changes in patients' ECG. By *semantic*, we refer to changes in the shape of the streaming data, which can change without obvious effect on statistical properties [@gharghabi2018].

The rest of this paper is organized as follows. In @sec-definitions, we provide a summary of the related work along with the necessary definitions. In @sec-matrixprofile we introduce the Matrix Profile framework. In @sec-semanticsegmentation, we introduce the FLOSS algorithm used for semantic segmentation. In @sec-methods, we describe how we will perform the regime change detection and its evaluation, including the datasets used, and the methodology for parameter analysis. In @sec-results, we present the results we obtained in each dataset. Finally, in @sec-discussion, we discuss the results and the potential impact of this work.

> In the @sec-conclusion we conclude.

# Definitions and notations

Here, we introduce the necessary definitions and terminology, beginning with the definition of a time series:

## Definitions {#sec-definitions}

::: {#def-ts}
: A time series $T \in \mathbb{R}^n$ is a sequence of real-valued numbers $t_i \in \mathbb{R}: T=\left[t_1, t_2, \ldots, t_n\right]$, in equally spaced time intervals where $n$ is the length of $T$.
:::

A local region of time series is called a subsequence (or a window) and is defined as follows:

::: {#def-subseq}
: A subsequence $T_{i,m} \in \mathbb{R}^m$ of a time series $T$ is a continuous subset of the values from $T$ of length $m$ starting from position $i$. Formally $T_{i, m}=\left[t_i, t_{i+1}, \ldots, t_{i+m-1}\right]$.
:::

If we take a subsequence and compute its distance to all subsequences in the same time series, we get a distance profile:

::: {#def-distprof}
: A distance profile $D \in \mathbb{R}^{n-m+1}$ of a time series $T$ and a subsequence $T_{i,m}$ is a vector stores $\operatorname{dist}\left(T_{i, m}, T_{j, m}\right) \forall j \in [1, 2, \ldots, n âˆ’ m + 1]$ , where $i \neq j$.
:::

Combining all distance profiles of a time series we get the matrix profile:

::: {#def-matrprof}
: A matrix profile $MP \in \mathbb{R}^{n-m+1}$ of a time series $T$ is a meta time series that stores the z-normalized Euclidean distance between each subsequence and its nearest neighbor where $n$ is the length of $T$ and $m$ is the given subsequence length.
:::

## Matrix Profile Background {#sec-matrixprofile}

Matrix Profile (MP) [@Yeh2017a], is a state-of-the-art [@DePaepe2020; @Feremans2020] time series analysis technique that once computed, allows us to derive frameworks to all sorts of tasks, as motif discovery, anomaly detection, regime change detection and others [@Yeh2017a].

Before MP, time series analysis relied on what is called *distance matrix* (DM), a matrix that stores all the distances between two time series (or itself, in case of a Self-Join). This was very power consuming, and several methods of pruning and dimensionality reduction were researched [@Lin2007].

For brevity, let's just understand that the MP and the companion Profile Index (PI) are two vectors that hold one floating point value and one integer value, respectively, regarding the original time series: (1) the similarity distance between that point on time (let's call these points "indexes") and its first nearest-neighbor (1-NN), (2) The index where this this 1-NN is located. The original paper has more detailed information [@Yeh2017a]. It is computed using a sliding window (@def-subseq) but instead of creating a whole DM, only the minimum values and the index of these minimum are stored (in the MP and PI respectively). We can have an idea of the relationship of both on @fig-thematrix .

![A distance matrix (top), and a matrix profile (bottom). The matrix profile stores only the minimum values of the distance matrix.](images/mp_1.png){#fig-thematrix}

> There are several ways to compute the MP. talk about the one suitable for small devices (MPX) that uses covariance instead of FFT, which is more suitable to constricted environment and short length TS. Code formulas are by Kaveh Kamgar. References Phillippe Pebay, Formulas for Robust, One-Pass Parallel Computation of Covariances and Arbitrary-Order Statistical Moments Knuth, semi-numerical algorithms, vol 2

## Semantic Segmentation {#sec-semanticsegmentation}

The regime change approach will be using the FLOSS (Fast Lowcost Online Semantic Segmentation) algorithm which uses the *Arc Counts* concept, as explained by Gharghabi, *et al.*,[@gharghabi2018].

The FLOSS algorithm is built on top of the Matrix Profile (MP)[@Yeh2017a], introduced above. Recalling that the MP and the companion Profile Index are two vectors holding information about the 1-NN. One can imagine several "arcs" starting from one "index" to another. This algorithm is based on the assumption that between two regimes, the most similar shape (its nearest neighbor) is located on the same *semantic* segment. As the number of "arcs" decreases, we can infer that the regime is changing (in this case the ECG waves, or another bio-signal). As show on @fig-arcsoriginal. This drop on the *Arc Counts* is a signal that a change on the shape of the signal has happened.

![FLOSS algorithm, using arc counts. The black dashed line shows a toy example where the arc count equals to four.](images/fluss_arcs.svg){#fig-arcsoriginal}

The choice of the FLOSS algorithm is founded on the following arguments:

-   **Domain Agnosticism:** the algorithm makes no assumptions about the data as opposed to most available algorithms to date.
-   **Streaming:** the algorithm can provide real-time information.
-   **Real-World Data Suitability:** the objective is not to *explain* all the data. Therefore, areas marked as "don't know" areas are acceptable.
-   **FLOSS is not:** a change point detection algorithm [@aminikhanghahi2016]. The interest here is changes in the shapes of a sequence of measurements.

Other algorithms we can cite are based on Hidden Markov Models (HMM) that require at least two parameters to be set by domain experts: cardinality and dimensionality reduction. The most attractive alternative could be the Autoplait [@Matsubara2014], which is also domain agnostic and parameter-free. It segments the time series using Minimum Description Length (MDL) and recursively tests if the region is best modeled by one or two HMM. However, Autoplait is designed for batch operation, not streaming, and also requires discrete data. FLOSS was demonstrated to be superior in several datasets in its original paper. In addition, FLOSS is robust to several changes in data like downsampling, bit depth reduction, baseline wandering, noise, smoothing, and even deleting 3% of the data and filling with simple interpolation. Finally, the most important, the algorithm is light and suitable for low-power devices.

The above cited researches [@kowsar2022; @bhanushali2020; @grammatikakis2021; @cheng2020] differ from this work on the following aspects: (1) the aim is to find and track any repeating pattern [@kowsar2022], (2) the aim is to infer a physiological condition of the patient, in batches ranging from 1 minute to 5 minutes, and uses a highly customized hardware to make the computations [@bhanushali2020], (3) the aim is to transmit the data to a remote server for processing [@grammatikakis2021], (4) the aim is to classify an arrhythmia, not to detect it; also uses compressed data and a pre-trained Neural Network [@cheng2020].

In the MP domain, it is worth also mentioning other possible algorithm: the Time Series Snippets [@Imani2018], based on MPdist [@gharghabi2018b]. The latter measures the distance between two sequences considering how many similar sub-sequences they share, no matter the order of matching. It proved to be a useful measure (not a metric) for meaningfully clustering similar sequences. Time Series Snippets exploits MPdist properties to summarize a dataset extracting the $k$ sequences that represent most of the data. The final result seems to be an alternative for detecting regime changes, but it is not. The purpose of this algorithm is to find which pattern(s) explains most of the dataset. Also, it is not suitable for streaming data. Lastly, MPdist is quite expensive compared to the trivial Euclidean distance.

> The article could then describe your algorithmic pipeline in detail, including the two main steps of regime change detection and classification. The article would conclude by discussing the results of your evaluation, as well as the potential impact of your work on the development of wearable arrhythmia detection devices.

# Methods and Materials {#sec-methods}

First let us introduce the concept of Nested Resampling [@Bischl2012]. It is known that when increasing model complexity, overfitting on the training set becomes more likely to happen [@Hastie2009]. This is an issue that this work has to countermeasure as there are many steps that requires parameter tuning, even for algorithms that are almost parameter-free like the MP.

The rule that must be followed is simple: *do not* evaluate a model on the same resampling split used to perform its own parameter tuning. Using simple cross-validation, the information about the test set "leaks" into the evaluation, which leads to overfitting/overtuning, and gives us an optimistic biased estimative of the performance. Bernd Bischl, 2012 [@Bischl2012] describes more deeply these factors, and also gives us a countermeasure for that: (1) from preprocessing the data to model selection use the training set; (2) the test set should be touched once, on the evaluation step; (3) repeat. This guarantees that a "new" separated data is only used *after* the model is trained/tuned.

@fig-nestedresampling shows us this principle. The steps (1) and (2) described above are part of the **Outer resampling**, which in each loop splits the data in two sets: the training set and the test set. The training set is then used in the **Inner resampling** where, for example, the usual cross-validation may be used (creating an *Analysis set* and an *Assessment set*, to avoid conflict of terminology), and the best model/parameters is selected. Then, this best model is evaluated against the unseen test set that was created for this resampling.

![Nested resampling. The full dataset is resampled several times (outer resampling), so each branch has its own Test set (yellow). On each branch, the Training set is used as if it were a full dataset, being resampled again (inner resampling); here the Assessment set (blue) is used to test the learning model and tune parameters. The best model then, is finally evaluated on its own Test set.](images/draw-nested-resampling.svg){#fig-nestedresampling}

The resulting (aggregated) performance of all outer samples gives us a more honest estimative of the expected performance on new data.

```{=html}
<!-- @fig-regimemodel shows the pipeline used on this paper to evaluate the regime change detection algorithm.

![Pipeline for regime change detection. The full dataset (containing several patients) is divided into a Training set and a Test set. The Training set is then resampled in an Analysis set and an Assessment set. The former is used for training/parameter tuning and the latter for assessing the result. The best parameters are then used for evaluation on the Test set. This may be repeated several times.](images/draw-regime-model.svg){#fig-regimemodel} -->
```
## Regime change detection {#sec-regimechange}

As we have seen previously, the FLOSS algorithm is built on top of the Matrix Profile (MP). Thus, we have proposed several parameters that may or not impact the FLOSS prediction performance. Worth to mention that the only parameter needed to compute the MP is the window size, the others were introduced by us to see if they could improve the prediction performance.

The variables for building the MP are:

-   **`window_size`**: the only parameter needed to compute the MP.
-   **`mp_threshold`**: the minimum similarity value to be considered for 1-NN.
-   **`time_constraint`**: the maximum distance to look for the nearest neighbor.

Later, the FLOSS algorithm also has parameters that can be tuned to optimize the prediction:

-   **`regime_threshold`**: the threshold below which a regime change is considered.
-   **`regime_landmark`**: the point in time where the regime threshold is applied (introduced later).

Using the `tidymodels` framework [@tidymodels2020] and the `targets` framework (for reproducibility) [@landau2021], we performed a basic grid search on all these parameters.

@fig-thepipeline shows the workflow using Nested resamplig in order to evaluate the FLOSS algorithm on the ECG datasets. @fig-flossregime shows a snapshot of the regime change detection on running pipeline. The graph on top shows the ECG streaming; the blue line marks the ten seconds before the original alarm was fired; the red line marks the time constraint used on the example; the dark red line marks the limit for taking a decision in this case of Asystole; The blue horizontal line represents the size of the sliding window. The graph on the middle shows the Arc counts as seen by the algorithm (with the corrected distribution); the red line marks the current minimum value and its index; the blue horizontal line shows the minimum value seen until then. The graph on the bottom shows the computed Arc counts (raw) and the red line is the theoretical distribution used for correction.

![Pipeline used to evaluate the FLOSS algorithm.](images/floss_net.svg){#fig-thepipeline fig-align="center"}

@fig-flossregime shows a snapshot of the regime change detection on running pipeline. The graph on top shows the last 60s of an ECG streaming. As we will explain later, the algorithm only stores a buffer of 20s of data. The green line marks the 10s window (which is a standard defined by ANSI/AAMI EC13 Cardiac Monitor Standards[@AAMI2002]) within the event must be located. I this example, the event is an Asystole that is technically defined as "No QRS for at least 4 seconds" (depicted by the red line). The yellow line(s) mark the detection of a regime change by FLOSS. The blue dash in the lower-right shows the size of the sliding window for a visual reference. The graph below shows the Arc counts as seen by the algorithm (with the corrected distribution); the red line marks the current minimum value.

![Regime change detection example. The graph on top shows the last 60s of an ECG streaming. As we will explain later, the algorithm only stores a buffer of 20s of data. The green line marks the 10s window (which is a standard defined by ANSI/AAMI EC13 Cardiac Monitor Standards[@AAMI2002]) within the event must be located. I this example, the event is an Asystole that is technically defined as "No QRS for at least 4 seconds" (depicted by the red line). The yellow line(s) mark the detection of a regime change by FLOSS. The blue dash in the lower-right shows the size of the sliding window for a visual reference. The graph below shows the Arc counts as seen by the algorithm (with the corrected distribution); the red line marks the current minimum value.](images/graph_regime_floss.svg){#fig-flossregime fig-align="center"}

| CAC on constraint: https://franzbischoff.github.io/false.alarm/blog-202110.html

## Datasets {#sec-datasets}

Althought the CinC/Physionet Challenge 2015 [@Clifford2015] has triggered the motivation for this work, the dataset provided by them is not suitable for showing the potential of the FLOSS algorithm.

In order to "overcharge" the FLOSS algorithm, we have chosen a dataset that contains several patients with paroxysmal atrial fibrillation events. The dataset used for working with the FLOSS algorithm was the "Paroxysmal Atrial Fibrillation Events Detection from Dynamic ECG Recordings: The 4th China Physiological Signal Challenge 2021" hosted by Zenodo [@bischoff2021afib] under the same license as Physionet.

### Atrial Fibrillation

The selected records were those that contain paroxysmal atrial fibrillation events, a total of 229 records. The records were split in a proportion of 3/4 for the training set (inner resampling) and 1/4 for the test set (outer resampling). The inner resampling was performed using a 5-fold cross-validation, which accounts for 137 records for fitting the models and 92 records for assessing them in the inner resampling.

The following parameters values were explored:

-   The MP parameters were explored using the following values:
    -   **`mp_threshold`**: 0.0 to 0.9, by 0.1 steps;
    -   **`time_constraint`**: 0 (no constraint), 800 and 1500;
    -   **`window_size`**: 25 to 150, by 25 steps;
-   The FLOSS parameters were explored using the following values:
    -   **`regime_threshold`**: 0.05 to 0.90, by 0.05 steps;
    -   **`regime_landmark`**: 1s to 10s, by 0.5 steps.

As we will see later, both `mp_threshold` and `time_constraint` will not impact the performance of the FLOSS algorithm. The `window_size`, `regime_threshold` and `regime_landmark` will be the only parameters that will be used on subsequent datasets to evaluate the FLOSS algorithm.

### Malignant Ventricular Ectopy

The second dataset was the "MIT-BIH Malignant Ventricular Ectopy Database" which contains 22 half-hour ECG recordings of subjects who experienced episodes of sustained ventricular tachycardia, ventricular flutter, and ventricular fibrillation, hosted by Zenodo [@bischoff2023malig] under the same license as Physionet.

The following parameters values were explored:

-   The MP parameters were explored using the following values:
    -   **`window_size`**: 25 to 200, by 25 steps;
-   The FLOSS parameters were explored using the following values:
    -   **`regime_threshold`**: 0.05 to 0.90, by 0.05 steps;
    -   **`regime_landmark`**: 2s to 9.5s, by 0.5 steps.

### Ventricular Tachyarrhythmia

The third dataset was the "CU Ventricular Tachyarrhythmia Database" which contains 35 eight-minute ECG recordings of human subjects who experienced episodes of sustained ventricular tachycardia, ventricular flutter, and ventricular fibrillation, hosted by Zenodo [@bischoff2023vtachy] under the same license as Physionet.

The following parameters values were explored:

-   The MP parameters were explored using the following values:
    -   **`window_size`**: 25 to 200, by 25 steps;
-   The FLOSS parameters were explored using the following values:
    -   **`regime_threshold`**: 0.05 to 0.90, by 0.05 steps;
    -   **`regime_landmark`**: 2s to 9.5s, by 0.5 steps.

## Parameters analysis {#sec-parameters}

The above methodology allows us to explore a wide combination of parameters, find the best model and reduces the chance of overfitting.

While this process is powerful and robust, it does not show us the importance of each parameter. This aspect is important for two reasons: (1) it allows us to reduce the search space for the next dataset, and (2) it allows us to understand the impact of each parameter on the model.

In order to check the effect of the parameters on the model, we need to compute the *importance* of each parameter.

Wei *et al.* published a comprehensive review on variable importance analysis [@Wei2015].

Our case is not a typical case of variable importance analysis, where a set of *features* are tested against an *outcome*. Instead, we have to proxy our analysis by using as *outcome* the FLOSS performance score and as *features* (or *predictors*) the tuning parameters that lead to that score.

That is accomplished by fitting a model using the tuning parameters to predict the FLOSS score and then applying the techniques to compute the importance of each parameter.

For this matter, a Bayesian Additive Regression Trees (BART) model was chosen after an experimental trial with a set of regression models (including glmnet, gbm, mlp) and for its inherent characteristics, which allows being used for model-free variable selection [@Chipman2010]. The best BART model was selected using 10-fold cross-validation repeated 3 times, having great predictive power with an RMSE around 0.2 and an R^2^ around 0.99. With this fitted model, we could evaluate each parameter's importance.

### Interactions

Before starting the parameter importance analysis, we need to consider the parameter interactions since this is usually the weak spot of the parameter analysis techniques.

For brevity, we will show an example of the model used in the first dataset. The first BART model was fitted using the following parameters:

$$
\begin{aligned}
E( score ) &= \alpha + time\_constraint\\
 &\quad + mp\_threshold + window\_size\\
 &\quad + regime\_threshold + regime\_landmark
\end{aligned}
$$ {#eq-first}

After checking the interactions, this is the refitted model:

$$
\begin{aligned}
E( score ) &= \alpha + time\_constraint\\
&\quad + mp\_threshold + window\_size\\
&\quad + regime\_threshold + regime\_landmark\\
&\quad + \left(regime\_threshold \times regime\_landmark\right)\\
&\quad + \left(mp\_threshold \times regime\_landmark\right)\\
&\quad + \left(mp\_threshold \times window\_size\right)
\end{aligned}
$$ {#eq-refitted}

### Importance

After evaluating the interactions, we then can perform the analysis of the variable importance. The goal is to understand how the FLOSS score behaves when we change the parameters.

Here is a brief overview of the different techniques:

#### Feature Importance Ranking Measure (FIRM)

The FIRM is a variance-based method. This implementation uses the ICE curves to quantify each feature effect which is more robust than partial dependence plots (PDP) [@Greenwell2020].

It is also helpful to inspect the ICE curves to uncover some heterogeneous relationships with the outcome [@Molnar2022].

**Advantages:**

-   Has a causal interpretation (for the model, not for the real world)
-   ICE curves can uncover heterogeneous relationships

**Disadvantages:**

-   The method does not take into account interactions.

#### Permutation

The Permutation method was introduced by Breiman in 2001 [@Breiman2001] for Random Forest, and the implementation used here is a model-agnostic version introduced by Fisher *et al.* in 2019 [@Fisher2018]. A feature is "unimportant" if shuffling its values leaves the model error unchanged, assuming that the model has ignored the feature for the prediction.

**Advantages:**

-   Easy interpretation: the importance is the increase in model error when the feature's information is destroyed.
-   No interactions: the interaction effects are also destroyed by permuting the feature values.

**Disadvantages:**

-   It is linked to the model error: not a disadvantage *per se*, but may lead to misinterpretation if the goal is to understand how the output varies, regardless of the model's performance. For example, if we want to measure the robustness of the model when someone tampers the features, we want to know the *model variance* explained by the features. Model variance (explained by the features) and feature importance correlate strongly when the model generalizes well (it is not overfitting).
-   Correlations: If features are correlated, the permutation feature importance can be biased by unrealistic data instances. Thus we need to be careful if there are strong correlations between features.

#### SHAP

The SHAP feature importance [@Lundberg2017] is an alternative to permutation feature importance. The difference between both is that Permutation feature importance is based on the decrease in model performance, while SHAP is based on the magnitude of feature attributions.

**Advantages:**

-   It is not linked to the model error: as the underlying concept of SHAP is the Shapley value, the value attributed to each feature is related to its contribution to the output value. If a feature is important, its addition will significantly affect the output.

**Disadvantages:**

-   Computer time: Shapley value is a computationally expensive method and usually is computed using Montecarlo simulations.
-   The Shapley value can be misinterpreted: The Shapley value of a feature value **is not** the difference of the predicted value after removing the feature from the model training. The interpretation of the Shapley value is: "Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value" [@Molnar2022].
-   Correlations: As with other permutation methods, the SHAP feature importance can be biased by unrealistic data instances when features are correlated.



Using the three techniques simultaneously allows a broad comparison of the model behavior [@Greenwell2020]. All three methods are model-agnostic (separates interpretation from the model), but as we have seen, each method has its advantages and disadvantages [@Molnar2022].

# Results {#sec-results}

### Interactions



@fig-interaction shows the variable interaction strength between pairs of variables. That allows us to verify if there are any significant interactions between the variables. Using the information from the first model fit, equation @eq-first, we see that `regime_threshold` interacts strongly with `regime_landmark`. This interaction was already expected, and we see that even after refitting the model, equation @eq-refitted, this interaction is still strong.

This is not a problem *per se* but a signal we must be aware of when exploring the parameters.

```{r fig-interaction, echo=FALSE, eval=TRUE, fig.height = 7, fig.width= 8, out.width="100%"}
#| fig-cap: "Variable interactions strength using feature importance ranking measure (FIRM) approach [@Greenwell2018].
#|  A) Shows strong interaction between `regime_threshold` and `regime_landmark`, `mp_threshold` and `window_size`,
#|     `mp_threshold` and `regime_landmark`.
#|  B) Refitting the model with these interactions taken into account, the strength is substantially reduced, except
#|     for the first, showing that indeed there is a strong correlation between those variables."

library(patchwork)

interactions <- readRDS(here::here("output", "importances_lmk.rds"))
importance_firm <- interactions$importance_firm
importance_perm <- interactions$importance_perm
importance_shap <- interactions$importance_shap
shap_html_test <- interactions$shap_html_test
shap_fastshap_all_test <- interactions$shap_fastshap_all_test
interactions <- interactions$interactions

interactions2 <- readRDS(here::here("output", "importances2_lmk.rds"))
importance_firm2 <- interactions2$importance_firm2
importance_perm2 <- interactions2$importance_perm2
importance_shap2 <- interactions2$importance_shap2
shap_fastshap_all_test2 <- interactions2$shap_fastshap_all_test2
shap_html_test2 <- interactions2$shap_html_test2
interactions2 <- interactions2$interactions2

interactions_plot <- ggplot2::ggplot(interactions, ggplot2::aes(
  x = reorder(Variables, Interaction),
  y = Interaction, fill = Variables
)) +
  ggplot2::geom_col(color = "grey35", linewidth = 0.2) +
  ggplot2::coord_flip() +
  ggplot2::labs(
    title = "Normal fit",
    y = ggplot2::element_blank(),
    x = ggplot2::element_blank()
  ) +
  ggplot2::ylim(0, 1.65) +
  ggplot2::theme_bw() +
  ggplot2::theme(legend.position = "none")

interactions2_plot <- ggplot2::ggplot(interactions2, ggplot2::aes(
  x = reorder(Variables, Interaction),
  y = Interaction, fill = Variables
)) +
  ggplot2::geom_col(color = "grey35", linewidth = 0.2) +
  ggplot2::coord_flip() +
  ggplot2::labs(
    title = "Taking into account the interactions",
    y = "Interaction strength",
    x = ggplot2::element_blank()
  ) +
  ggplot2::ylim(0, 1.65) +
  ggplot2::theme_bw() +
  ggplot2::theme(legend.position = "none")

inter <- interactions_plot / interactions2_plot
inter + plot_annotation(
  title = "Variable Interaction Strength",
  tag_levels = c("A", "1"),
  theme = ggplot2::theme_bw()
)
```



### Importance analysis



@fig-importance then shows the variable importance using three methods: Feature Importance Ranking Measure (FIRM) using Individual Conditional Expectation (ICE), Permutation-based, and Shapley Additive explanations (SHAP). The first line of this figure shows an interesting result that probably comes from the main disadvantage of the FIRM method: *the method does not take into account interactions*. We see that FIRM is the only one that disagrees with the other two methods, giving much importance to `window_size`.

In the second line, taking into account the interactions, we see that all methods somewhat agree with each other, accentuating the importance of `regime_threshold`, which makes sense as it is the most evident parameter we need to set to determine if the *Arc Counts* are low enough to indicate a regime change.

```{r fig-importance, eval=TRUE, echo=FALSE, warning=FALSE, fig.height = 7, fig.width= 15, out.width="100%"}
#| fig-cap: "Variables importances using three different methods. A) Feature Importance Ranking Measure
#|  using ICE curves. B) Permutation method. C) SHAP (100 iterations). Line 1 refers to the original
#|  fit, and line 2 to the re-fit, taking into account the interactions between variables
#|  (@fig-interaction)."

library(patchwork)

importance_firm_plot <- ggplot2::ggplot(importance_firm, ggplot2::aes(
  x = reorder(Variable, Importance),
  y = Importance, fill = Variable
)) +
  ggplot2::geom_col(colour = "grey35", linewidth = 0.8, show.legend = FALSE) +
  ggplot2::coord_flip() +
  ggplot2::labs(
    title = "Feature Importance Ranking Measure",
    subtitle = "Individual Conditional Expectation",
    x = "",
    y = ggplot2::element_blank()
  ) +
  ggplot2::ylim(0, 2.2) +
  ggplot2::theme_bw() +
  ggplot2::theme(
    legend.position = "none",
    plot.margin = ggplot2::margin(0, 0, 0, 10)
  )

importance_perm_plot <- ggplot2::ggplot(importance_perm, ggplot2::aes(
  x = reorder(Variable, Importance),
  y = Importance, fill = Variable
)) +
  ggplot2::geom_boxplot(colour = "grey35", linewidth = 0.3, show.legend = FALSE) +
  ggplot2::coord_flip() +
  ggplot2::labs(
    title = "Permutation-based (100x)",
    x = "",
    y = ggplot2::element_blank()
  ) +
  ggplot2::ylim(3, 20) +
  ggplot2::theme_bw() +
  ggplot2::theme(
    legend.position = "none",
    plot.margin = ggplot2::margin(0, 0, 0, 10)
  )

importance_shap_plot <- ggplot2::ggplot(importance_shap, ggplot2::aes(
  x = reorder(Variable, Importance),
  y = Importance, fill = Variable
)) +
  ggplot2::geom_col(colour = "grey35", linewidth = 0.8, show.legend = FALSE) +
  ggplot2::coord_flip() +
  ggplot2::labs(
    title = "SHAP (100 iterations)",
    x = "",
    y = ggplot2::element_blank()
  ) +
  ggplot2::ylim(0, 1.5) +
  ggplot2::theme_bw() +
  ggplot2::theme(
    legend.position = "none",
    plot.margin = ggplot2::margin(0, 0, 0, 10)
  )

importance_firm2_plot <- ggplot2::ggplot(importance_firm2, ggplot2::aes(
  x = reorder(Variable, Importance),
  y = Importance, fill = Variable
)) +
  ggplot2::geom_col(colour = "grey35", linewidth = 0.8, show.legend = FALSE) +
  ggplot2::coord_flip() +
  ggplot2::labs(
    x = "",
    y = "Importance"
  ) +
  ggplot2::ylim(0, 2.2) +
  ggplot2::theme_bw() +
  ggplot2::theme(
    legend.position = "none",
    plot.margin = ggplot2::margin(0, 0, 0, 10)
  )

importance_perm2_plot <- ggplot2::ggplot(importance_perm2, ggplot2::aes(
  x = reorder(Variable, Importance),
  y = Importance, fill = Variable
)) +
  ggplot2::geom_boxplot(colour = "grey35", linewidth = 0.3, show.legend = FALSE) +
  ggplot2::coord_flip() +
  ggplot2::labs(
    x = "",
    y = "Importance"
  ) +
  ggplot2::ylim(3, 20) +
  ggplot2::theme_bw() +
  ggplot2::theme(
    legend.position = "none",
    plot.margin = ggplot2::margin(0, 0, 0, 10)
  )

importance_shap2_plot <- ggplot2::ggplot(importance_shap2, ggplot2::aes(
  x = reorder(Variable, Importance),
  y = Importance, fill = Variable
)) +
  ggplot2::geom_col(colour = "grey35", linewidth = 0.8, show.legend = FALSE) +
  ggplot2::coord_flip() +
  ggplot2::labs(
    x = "",
    y = "Importance"
  ) +
  ggplot2::ylim(0, 1.5) +
  ggplot2::theme_bw() +
  ggplot2::theme(
    legend.position = "none",
    plot.margin = ggplot2::margin(0, 0, 0, 10)
  )


all <- (importance_firm_plot / importance_firm2_plot + plot_layout(tag_level = "new")) |
  (importance_perm_plot / importance_perm2_plot + plot_layout(tag_level = "new")) |
  (importance_shap_plot / importance_shap2_plot + plot_layout(tag_level = "new")) +
    plot_layout(guides = "collect")
all + plot_annotation(
  title = "Variable importances",
  tag_levels = c("A", "1"),
  theme = ggplot2::theme_bw() + ggplot2::theme(
    plot.title = ggplot2::element_text(size = 20)
  )
)
```

@fig-importanceshap and @fig-importanceshap2 show the effect of each feature on the FLOSS score. The more evident difference is the shape of the effect of `time_constraint` that initially suggested better results with larger values. However, removing the interactions seems to be a flat line.

Based on @fig-importance and @fig-importanceshap2 we can infer that:

-   **`regime_threshold`**: is the most important feature, has an optimal value to be set, and since the high interaction with the `regime_landmark`, both must be tuned simultaneously. In this setting, high thresholds significantly impact the score, probably due to an increase in false positives starting on \>0.65 the overall impact is mostly negative.

-   **`regime_landmark`**: is not as important as the `regime_threshold,` but since there is a high interaction, it must not be underestimated. It is known that the *Arc Counts* have more uncertainty as we approach the margin of the streaming, and this becomes evident looking at how the score is negatively affected for values below 3.5s.

-   **`window_size`**: has a near zero impact on the score when correctly set. Nevertheless, for higher window values, the score is negatively affected. This high value probably depends on the data domain. In this setting, the model is being tuned towards the changes from atrial fibrillation/non-fibrillation; thus, the "shape of interest" is small compared to the whole heartbeat waveform. Window sizes smaller than 150 are more suitable in this case. As Beyer *et al.* noted, "as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point" [@Beyer1999], which means that the bigger the window size, the smaller will be the contrast between different regimes.

-   **`mp_threshold`**: has a fair impact on the score, but primarily by *not using it*. We start to see a negative impact on the score with values above 0.60, while a constant positive impact with lower values.

-   **`time_constraint`**: is a parameter that must be interpreted cautiously. The 0 (zero) value means **no constraint**, which is equivalent to the size of the FLOSS history buffer (in our setting, 5000). We can see that this parameter's impact throughout the possible values is constantly near zero.

In short, for the MP computation, the parameter that is worth tuning is the `window_size`, while for the FLOSS computation, both `regime_threshold` (mainly) and `regime_landmark` shall be tuned.

```{r fig-importanceshap, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height = 6, fig.width= 10, out.width="100%"}
#| fig-cap: "This shows the effect each variable has on the FLOSS score. This plot doesn't take into account the
#|  variable interactions."

library(dplyr)
library(patchwork)

trained_model <- readRDS(here::here("output", "dbarts_fitted_lmk.rds"))
train_data <- trained_model$training_data
testing_data <- trained_model$testing_data
predictors_names <- c("time_constraint", "regime_threshold", "mp_threshold", "window_size", "regime_landmark")
outcome_name <- "mean"

testing_data2 <- testing_data |>
  dplyr::mutate(
    int_rt_rl = regime_threshold * regime_landmark,
    int_mp_w = mp_threshold * window_size,
    int_mp_rl = mp_threshold * regime_landmark,
    # int_mp_rt = mp_threshold * regime_threshold
    # int_mp_tc = mp_threshold * time_constraint
    .before = mean
  )

layout <- "
AABB
CCDD
#EE#
"
# 5% has acceptable variability for not using the whole datasets
all_data <- dplyr::bind_rows(train_data, testing_data)
split <- all_data |> dplyr::filter(time_constraint %in% c(0, 800, 1500), mp_threshold %in% c(0.00, 0.4, 0.6, 0.8), window_size <= 150)
set.seed(19746)
sample_data <- dplyr::slice_sample(split, n = 3, by = 1:4)
ttdata <- sample_data[, predictors_names]
base_data <- sample_data[, outcome_name]$mean

d1 <- shapviz::shapviz(shap_fastshap_all_test, X = ttdata[, predictors_names], baseline = mean(base_data))
a1 <- shapviz::sv_dependence(d1, "window_size", color_var = "auto") +
  ggplot2::geom_smooth(method = loess, colour = "#0000ff44", alpha = 0.2) +
  ggplot2::labs(y = ggplot2::element_blank()) +
  ggplot2::theme_bw()
a2 <- shapviz::sv_dependence(d1, "regime_threshold", color_var = "auto") +
  ggplot2::geom_smooth(method = loess, colour = "#0000ff44", alpha = 0.2) +
  ggplot2::labs(y = ggplot2::element_blank()) +
  ggplot2::theme_bw()
a3 <- shapviz::sv_dependence(d1, "regime_landmark", color_var = "auto") +
  ggplot2::geom_smooth(method = loess, colour = "#0000ff44", alpha = 0.2) +
  ggplot2::labs(y = ggplot2::element_blank()) +
  ggplot2::theme_bw()
a4 <- shapviz::sv_dependence(d1, "mp_threshold", color_var = "auto") +
  # ggplot2::geom_smooth(method = loess, colour = "#0000ff44", alpha = 0.2) +
  ggplot2::labs(y = ggplot2::element_blank()) +
  ggplot2::theme_bw()
a5 <- shapviz::sv_dependence(d1, "time_constraint", color_var = "auto") +
  ggplot2::geom_smooth(method = loess, colour = "#0000ff44", alpha = 0.2) +
  ggplot2::labs(y = ggplot2::element_blank()) +
  ggplot2::theme_bw()

all1 <- ((a1 + a5 + a3 + a4 + plot_layout(guides = "collect")) + (a2 + plot_layout(guides = "collect")))
all1 + plot_layout(design = layout) + plot_annotation(title = "Shapley value vs. variable values", theme = ggplot2::theme_bw())
```

```{r fig-importanceshap2, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height = 6, fig.width= 10, out.width="100%"}
#| fig-cap: "This shows the effect each variable has on the FLOSS score, taking into account the interactions."

# t1 <- autoplot(shap_fastshap_all_test2,
#   type = "dependence",
#   X = testing_data2, feature = predictors_names[2], alpha = 0.2
# ) + ggplot2::geom_smooth(method = loess) + ggplot2::labs(y = ggplot2::element_blank()) + ggplot2::theme_bw()
# t2 <- autoplot(shap_fastshap_all_test2,
#   type = "dependence",
#   X = testing_data2, feature = predictors_names[1], alpha = 0.2
# ) + ggplot2::geom_smooth(method = loess) + ggplot2::labs(y = ggplot2::element_blank()) + ggplot2::theme_bw()
# t3 <- autoplot(shap_fastshap_all_test2,
#   type = "dependence",
#   X = testing_data2, feature = predictors_names[4], alpha = 0.2
# ) + ggplot2::geom_smooth(method = loess) + ggplot2::labs(y = ggplot2::element_blank()) + ggplot2::theme_bw()
# t4 <- autoplot(shap_fastshap_all_test2,
#   type = "dependence",
#   X = testing_data2, feature = predictors_names[3], alpha = 0.2
# ) + ggplot2::geom_smooth(method = loess) + ggplot2::labs(y = ggplot2::element_blank()) + ggplot2::theme_bw()
# t5 <- autoplot(shap_fastshap_all_test,
#   type = "dependence",
#   X = testing_data2, feature = predictors_names[5], alpha = 0.2
# ) + ggplot2::geom_smooth(method = loess) + ggplot2::labs(y = ggplot2::element_blank()) + ggplot2::theme_bw()

layout <- "
AABB
CCDD
#EE#
"
# 5% has acceptable variability for not using the whole datasets
ttdata2 <- sample_data[, predictors_names]
base_data <- sample_data[, outcome_name]$mean

d1 <- shapviz::shapviz(shap_fastshap_all_test2, X = ttdata2[, predictors_names], baseline = mean(base_data))
a1 <- shapviz::sv_dependence(d1, "window_size", color_var = "auto") +
  ggplot2::geom_smooth(method = loess, colour = "#0000ff44", alpha = 0.2) +
  ggplot2::labs(y = ggplot2::element_blank()) +
  ggplot2::theme_bw()
a2 <- shapviz::sv_dependence(d1, "regime_threshold", color_var = "auto") +
  ggplot2::geom_smooth(method = loess, colour = "#0000ff44", alpha = 0.2) +
  ggplot2::labs(y = ggplot2::element_blank()) +
  ggplot2::theme_bw()
a3 <- shapviz::sv_dependence(d1, "regime_landmark", color_var = "auto") +
  ggplot2::geom_smooth(method = loess, colour = "#0000ff44", alpha = 0.2) +
  ggplot2::labs(y = ggplot2::element_blank()) +
  ggplot2::theme_bw()
a4 <- shapviz::sv_dependence(d1, "mp_threshold", color_var = "auto") +
  # ggplot2::geom_smooth(method = loess, colour = "#0000ff44", alpha = 0.2) +
  ggplot2::labs(y = ggplot2::element_blank()) +
  ggplot2::theme_bw()
a5 <- shapviz::sv_dependence(d1, "time_constraint", color_var = "auto") +
  ggplot2::geom_smooth(method = loess, colour = "#0000ff44", alpha = 0.2) +
  ggplot2::labs(y = ggplot2::element_blank()) +
  ggplot2::theme_bw()

all2 <- ((a1 + a5 + a3 + a4 + plot_layout(guides = "collect")) + (a2 + plot_layout(guides = "collect")))
all2 + plot_layout(design = layout) + plot_annotation(title = "Shapley value vs. variable values", theme = ggplot2::theme_bw())
```

According to the FLOSS paper [@gharghabi2018], the `window_size` is indeed a feature that can be tuned; nevertheless, the results appear to be similar in a reasonably wide range of window sizes, up to a limit, consistent with our findings.

### Visualizing the predictions

At this point, the grid search tested a total of 23,389 models with resulting (individual) scores from 0.0002 to 1669.83 (Q25: 0.9838, Q50: 1.8093, Q75: 3.3890).

#### By recording

First, we will visualize how the models (in general) performed throughout the individual recordings.

@fig-global shows a violin plot of equal areas clipped to the minimum value. The blue color indicates the recordings with a small IQR (interquartile range) of model scores. We see on the left half 10% of the recordings with the worst minimum score, and on the right half, 10% of the recordings with the best minimum score.

Next, we will visualize some of these predictions to understand why some recordings were difficult to segment. For us to have a simple baseline: a recording with just one regime change, and the model predicts exactly one regime change, but far from the truth, the score will be roughly 1.

```{r fig-global, eval=TRUE, echo=FALSE, fig.height=5, fig.width=10, out.width="100%"}
#| fig-cap: "Violin plot showing the distribution of the FLOSS score achieved by all tested models by
#|  recording.  The left half shows the recordings that were difficult to predict (10% overall), whereas
#|  the right half shows the recordings that at least one model could achieve a good prediction (10%
#|  overall).  The recordings are sorted (left-right) by the minimum (best) score achieved in descending
#|  order, and ties are sorted by the median of all recording scores.  The blue color highlights
#|  recordings where models had an IQR variability of less than one.  As a simple example, a recording
#|  with just one regime change, and the model predicts exactly one change, far from the truth, the
#|  score will be roughly 1."

all_scores <- readRDS(here::here("output", "regime_outputs_lmk.rds"))

scores_stats <- all_scores |>
  dplyr::select(record, score) |>
  dplyr::group_by(record) |>
  dplyr::reframe(
    score = score, min = min(score), q25 = quantile(score, 0.25),
    median = quantile(score, 0.5), q75 = quantile(score, 0.75),
    mean = mean(score), max = max(score)
  )

records_factors <- forcats::as_factor(scores_stats$record)

scores_stats$id <- sprintf("%03d", (as.numeric(records_factors)))

scores_stats |>
  dplyr::mutate(low_iqr = q75 - q25 < 1) |>
  dplyr::filter(min > quantile(min, 0.9) | min < quantile(min, 0.1)) |>
  ggplot2::ggplot(ggplot2::aes(x = reorder(reorder(id, -median), -min), y = score, colour = low_iqr)) +
  ggplot2::geom_violin() +
  ggplot2::coord_cartesian(ylim = c(0, 5)) +
  ggplot2::theme_bw() +
  ggplot2::labs(title = "Scores by recording", colour = "IQR < 1", x = "Recording ID", y = "Score distribution")
```

@fig-worst shows the best effort in predicting the most complex recordings. One information not declared before is that if the model does not predict any change, it will put a mark on the zero position. On the other side, the truth markers positioned at the beginning and the end of the recording were removed, as these locations lack information and do not represent a streaming setting.

```{r fig-worst, eval=TRUE, echo=FALSE, fig.height=15, fig.width=12, out.width="100%", dev="png"}
#| fig-cap: "Prediction of the worst 10% of recordings (red is the truth, blue are the predictions)."

worst <- scores_stats |>
  dplyr::filter(min > quantile(min, 0.9)) |>
  dplyr::group_by(record) |>
  dplyr::slice_head() |>
  dplyr::ungroup() |>
  dplyr::arrange(desc(min), desc(median)) |>
  dplyr::slice_head(n = 10) |>
  dplyr::pull(record)
# "data_25_1.par"  "data_32_12.par" "data_85_1.par"  "data_90_1.par"  "data_68_2.par"

worst_data <- all_scores |>
  dplyr::filter(record %in% worst) |>
  dplyr::group_by(record) |>
  dplyr::slice_min(n = 1, order_by = score, with_ties = FALSE) |>
  dplyr::arrange(desc(score)) |>
  dplyr::ungroup()

plots <- list()
for (i in seq_len(nrow(worst_data))) {
  plots[[i]] <- tkplot(worst_data[i, ], FALSE, 50, dataset = "afib")
}

wrap_plots(plots, ncol = 1)
```

@fig-best shows the best performances of the best recordings. Notice that there are recordings with a significant duration and few regime changes, making it hard for a "trivial model" to predict randomly.

```{r fig-best, eval=TRUE, echo=FALSE, fig.height=15, fig.width=12, out.width="100%", dev="png"}
#| fig-cap: "Prediction of the best 10% of recordings (red is the truth, blue are the predictions)."

bests <- scores_stats |>
  dplyr::filter(min < quantile(min, 0.1)) |>
  dplyr::group_by(record) |>
  dplyr::slice_head() |>
  dplyr::ungroup() |>
  dplyr::arrange(desc(min), desc(median)) |>
  dplyr::slice_head(n = 10) |>
  dplyr::pull(record)

bests_data <- all_scores |>
  dplyr::filter(record %in% bests) |>
  dplyr::group_by(record) |>
  dplyr::slice_min(n = 1, order_by = score, with_ties = FALSE) |>
  dplyr::arrange(desc(score)) |>
  dplyr::ungroup()

plots <- list()
for (i in seq_len(nrow(bests_data))) {
  plots[[i]] <- tkplot(bests_data[i, ], FALSE, 50, dataset = "afib")
}

wrap_plots(plots, ncol = 1)
```

#### By model

@fig-globalmodel shows the distribution of the FLOSS score of the 10% worst (left side) and 10% best models across the recordings (right side). The bluish color highlights the models with SD below 3 and IQR below 1.

```{r fig-globalmodel, eval=TRUE, echo=FALSE, fig.height=5, fig.width=10, out.width="100%"}
#| fig-cap: "Violin plot showing the distribution of the FLOSS score achieved by all tested models during the
#|  inner ressample.  The left half shows the models with the worst performances (10% overall), whereas
#|  the right half shows the models with the best performances (10% overall).
#|  The models are sorted (left-right) by the mean score (top) and by the median (below). Ties are
#|  sorted by the SD and IQR, respectively.  The bluish colors highlights models with an SD below 3
#|  and IQR below 1."

# cores_stats_model <- all_scores |> dplyr::mutate(across(all_of(predictors_names), as.factor, .unpack = FALSE))

if (file.exists(here::here("output", "scores_stats_model_rep.rds"))) {
  scores_stats_model <- readRDS(here::here("output", "scores_stats_model_rep.rds"))
} else {
  scores_stats_model <- all_scores |>
    dplyr::group_by(dplyr::across(dplyr::all_of(predictors_names))) |>
    dplyr::mutate(model = glue::glue("{window_size}_{time_constraint}_{mp_threshold}_{regime_threshold}_{regime_landmark}")) |>
    dplyr::reframe(
      record = record,
      model = model,
      score = score, min = min(score), q25 = quantile(score, 0.25),
      median = quantile(score, 0.5), q75 = quantile(score, 0.75),
      iqr = q75 - q25,
      mean = mean(score), max = max(score),
      sd = sd(score)
    )
  saveRDS(scores_stats_model, file = here::here("output", "scores_stats_model_rep.rds"))
}


scores_stats_model$id <- (sprintf("%05d", (as.numeric(as.factor(scores_stats_model$model)))))
scores_stats_model$id_text <- (sprintf("Model_%05d", (as.numeric(as.factor(scores_stats_model$model)))))
scores_stats_model$record <- (sprintf("%03d", (as.numeric(factor(scores_stats_model$record, labels = levels(records_factors))))))
scores_stats_model <- scores_stats_model |> dplyr::select(-model)

low <- head(sort(unique(scores_stats_model$mean)), 20)
high <- tail(sort(unique(scores_stats_model$mean)), 20)

model_mean <- scores_stats_model |>
  dplyr::mutate(low_sd = sd < 3) |>
  dplyr::filter(mean > high | mean < low) |>
  ggplot2::ggplot(ggplot2::aes(x = reorder(reorder(id, -sd), -mean), y = score, colour = low_sd)) +
  ggplot2::scale_colour_manual(values = c("FALSE" = "#ff0000c2", "TRUE" = "#0000ffb5")) +
  ggplot2::geom_violin() +
  ggplot2::coord_cartesian(ylim = c(0, 3)) +
  ggplot2::theme_bw() +
  ggplot2::theme(axis.text.x = ggplot2::element_text(size = 8, angle = 90, vjust = 0.5, hjust = 1)) +
  ggplot2::labs(subtitle = "Ordered by Mean and SD", colour = "SD < 3", x = ggplot2::element_blank(), y = "Score distribution")

low <- head(sort(unique(scores_stats_model$median)), 20)
high <- tail(sort(unique(scores_stats_model$median)), 20)

model_median <- scores_stats_model |>
  dplyr::mutate(low_iqr = q75 - q25 < 1) |>
  dplyr::filter(median > high | median < low) |>
  ggplot2::ggplot(ggplot2::aes(x = reorder(reorder(id, -iqr), -median), y = score, colour = low_iqr)) +
  ggplot2::geom_violin() +
  ggplot2::coord_cartesian(ylim = c(0, 3)) +
  ggplot2::theme_bw() +
  ggplot2::theme(axis.text.x = ggplot2::element_text(size = 8, angle = 90, vjust = 0.5, hjust = 1)) +
  ggplot2::labs(subtitle = "Ordered by Median and IQR", colour = "IQR < 1", x = "Model ID", y = "Score distribution")

(model_mean / model_median) + plot_layout(guides = "auto") +
  plot_annotation(
    title = "Scores grouped by model",
    theme = ggplot2::theme_bw()
  )
```

@fig-bestmodels the performance of the six best models. They are ordered from left to right, from the worst record to the best record. The top model is the one with the lowest mean across the scores. The red line indicates the median score. The scores above 3 are squished in the plot and colored according to the scale in the legend.

```{r fig-bestmodels, eval=TRUE, echo=FALSE, fig.height=22, fig.width=18, out.width="100%"}
#| fig-cap: "Performances of the best 6 models across all inner resample of recordings.
#|  The recordings are ordered by score, from the worst to the best.
#|  Each plot shows one model, starting from the best one.
#|  The red line indicates the median score of the model.
#|  The gray line limits the zero-score region. The plot is limited on the \"y\" axis, and the scores above this
#|  limit are shown in color."

best_models <- scores_stats_model |>
  dplyr::filter(mean < quantile(mean, 0.1)) |>
  dplyr::arrange(mean, sd) |>
  dplyr::pull(id_text) |>
  unique()
best_models <- best_models[1:6]

plots <- list()
for (i in seq_len(length(best_models))) {
  dd <- scores_stats_model |> dplyr::filter(id_text == best_models[i])
  plots[[i]] <- ggplot2::ggplot(dd, ggplot2::aes(x = reorder(record, -score), y = score, colour = score)) +
    ggplot2::geom_point(size = 2) +
    ggplot2::geom_hline(ggplot2::aes(yintercept = median), colour = "red") +
    # ggplot2::geom_hline(ggplot2::aes(yintercept = mean), colour = "blue") +
    ggplot2::geom_hline(ggplot2::aes(yintercept = 0), colour = "gray50") +
    ggplot2::scale_y_continuous(
      limits = c(0, 3),
      oob = scales::oob_squish,
      expand = c(0.1, 0.05, 0.1, -0.1)
    ) +
    ggplot2::scale_color_gradientn(colors = c("#ffd500db", "#ff8800", "#ff5e00", "#ff0000"), limits = c(3.1, 100)) +
    ggplot2::theme_bw(base_size = 15) +
    ggplot2::theme(axis.text.x = ggplot2::element_text(size = 9, angle = 90, vjust = 0.5, hjust = 1)) +
    ggplot2::labs(
      title = best_models[i],
      colour = "Score out\nof bounds",
      x = ifelse(i == length(best_models), "Record ID", ""),
      y = ggplot2::element_blank()
    )
}

wrap_plots(plots, ncol = 1, guides = "collect") + plot_annotation(
  title = "Performances of the 6 best models",
  theme = ggplot2::theme_bw()
)

```

#### The Holdout

Finally, @tbl-bestparam shows a summary of the best five models across all the inner resample (cross-validation). The column `mean` shows the average score, and column `std_err` shows the standard error of the mean. The column `holdout` shows the final score of this model on the holdout set (outer resample).

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
#| label: tbl-bestparam
#| tbl-cap: "Summary of the five best models. The `mean` shows the inner resample average score. The `holdout` shows
#|  the final score of the model on the holdout set (outer resample)."

if (file.exists(here::here("output", "regime_outputs_holdout.rds"))) {
  regime_outputs_holdout <- readRDS(here::here("output", "regime_outputs_holdout.rds"))

  best_parameters <- regime_outputs_holdout$combined |>
    dplyr::group_by(
      window_size, regime_threshold, regime_landmark, .metric, .estimator
    ) |>
    dplyr::summarise(mean = mean(.estimate), std_err = sd(.estimate), n = dplyr::n()) |>
    dplyr::ungroup() |>
    dplyr::arrange(mean, std_err) |>
    dplyr::slice_min(n = 5, order_by = mean) |>
    dplyr::select(-.metric, -.estimator, -n)

  holdout <- regime_outputs_holdout$results |>
    dplyr::select(.estimate) |>
    dplyr::rename(holdout = .estimate)

  best_parameters <- dplyr::bind_cols(best_parameters, holdout)

  kableExtra::kable(best_parameters,
    booktabs = TRUE,
    longtable = TRUE,
    align = "cccccc",
    digits = 2,
    position = "ht",
    linesep = ""
  ) |>
    kableExtra::row_spec(0, bold = TRUE) |>
    kableExtra::column_spec(6, bold = TRUE)
}
```

Presentation of the test cases and evaluation metrics, showcasing the effectiveness of the approach.

Results of the detection. Comparison with existing methods. Validity of the system on different datasets, including Physionet Challenges.

Performance of the FLOSS algorithm in detecting ECG signal changes Comparison with other existing techniques, if any

# Discussion {#sec-discussion}

Comparison with existing techniques, potential applications, and future directions.

Benefits of the proposed system. Potential applications in healthcare and wearable technology. Future enhancements and integrations.

Implications of the results Challenges faced and potential improvements

# Conclusion and Future Work {#sec-conclusion}

Summary of the article and the potential of integrating it with the classification system discussed in the second article.

Summary of the findings and the potential impacts.
